---
title: "Project 2"
author: "Daniel Fredin, Junhan Li, & Eric Chen"
output: pdf_document
---

```{r include=FALSE, echo=FALSE}
# Libraries
library(tidyverse)
library(ggplot2)
library(dplyr)
library(splitstackshape)
library(readr)
library(car)
library(lmtest)
library(rcompanion)
library(olsrr)
library(kableExtra)
library(caret)
```




# Problem 1

```{r include=FALSE, echo=FALSE}
glass <- read.csv("glass.csv")
```






# Problem 3

## a)
```{r include=FALSE, echo=FALSE}
Handout1 <- read.csv("Handout 1.csv")
```


### ii) 10-fold Cross-Validation

```{r include=FALSE, echo=FALSE}
# Convert COMMIT to binary < median = 0, otherwise = 1.

new_Handout1 <- Handout1 %>%
  mutate(COMMIT = case_when(
    (COMMIT < median(COMMIT)) ~ 0,
    TRUE ~ 1))

# Select only the continuous numerical variables
new_Handout1 <- new_Handout1 %>%
  select(COMMIT, AGE, SALARY, CLASSSIZE, RESOURCES, AUTONOMY, CLIMATE, SUPPORT)


# Training and testing method
set.seed(123)
# Create training and testing data
index <- createDataPartition(new_Handout1$COMMIT, 
                             p = 0.8, 
                             list = FALSE,
                             times=1)


new_Handout1 <- as.data.frame(new_Handout1)

train  <- new_Handout1[index, ]
test <- new_Handout1[-index, ]

train$COMMIT[train$COMMIT==1] <- "yes"
train$COMMIT[train$COMMIT==0] <- "no"

test$COMMIT[test$COMMIT==1] <- "yes"
test$COMMIT[test$COMMIT==0] <- "no"


# Convert outcome variable to factor for each data frame
train$COMMIT <- as.factor(train$COMMIT)
test$COMMIT <- as.factor(test$COMMIT)

# 10-fold Cross-validation method
ctrlspecs <- trainControl(method="cv", 
                          number=10, 
                          savePredictions="all",
                          classProbs=TRUE)

set.seed(1234)
cvmodel <- train(COMMIT ~ .,  
                 data=train, 
                 method="glm", 
                 family = "binomial",  
                 trControl=ctrlspecs)


# results1 <- data.frame( R2 = R2(predictions, test$COMMIT),
#             RMSE = RMSE(predictions, test$COMMIT),
#             MAE = MAE(predictions, test$COMMIT))
# # Results for 10-fold Cross-validation method
# results1

# Model for 10-fold Cross-validation method
summary(cvmodel)
print(cvmodel)
varImp(cvmodel)

```

### Model Accuracy

```{r}
# Predict outcome using model from training data based on testing data
predictions <- predict(cvmodel, newdata=test)

# Create confusion matrix to assess model fit/performance on test data
con_matx <- confusionMatrix(data=predictions, test$COMMIT)
con_matx

```


## b)



## c)

```{r}
compute_weibull_stats <- function(shape, scale) {
  # Compute the mean
  mean_value <- scale * gamma(1 + (1/shape))
  
  
  # # Compute the median
  # median_value <- scale * qweibull(0.5, shape, scale)
  
  
  # Compute the median
  median_value <- scale * (log(2)^(1/shape))
  
  
  # Compute the mode
  if (shape > 1)
    mode_value <- scale * ((shape - 1) / shape)^(1/shape)
  else
    mode_value <- 0
  
  # Compute the variance
  variance_value <- scale^2 * (gamma(1 + (2/shape)) - (gamma(1 + (1/shape)))^2)
  
  # Return the computed statistics as a named list
  return(list(mean = mean_value, 
              median = median_value, 
              mode = mode_value, 
              variance = variance_value))
}

# lambda = scale
# K = shape

# Example usage
scale_param <- 6
shape_param <- 1

stats <- compute_weibull_stats(shape_param, scale_param)

# Accessing the computed statistics
mean_value <- stats$mean
median_value <- stats$median
mode_value <- stats$mode
variance_value <- stats$variance

# Printing the computed statistics
cat("Mean:", mean_value, "\n")
cat("Median:", median_value, "\n")
cat("Mode:", mode_value, "\n")
cat("Variance:", variance_value, "\n")

```

### Maximum likelihood estimator (MLE) of the parameters from the Weibull distribution

Maximum Likelihood Estimator (MLE) of $\sigma$ from the normal distribution assuming $\mu$ is constant.


### Step 1: Probability Density Function for a normal distribution.


$$
\Large f(x;\lambda,k) = 
\begin{cases}
  \Large\frac{k}{\lambda}\left(\frac{x}{\lambda}\right)^{k-1}  e^{-\left(\frac{x}{\lambda}\right)^k}  , & x \ge 0\\
  \Large0, &x < 0
\end{cases}
$$


### Step 2: The likelihood function (L).


$$
\begin{aligned}
 \Large L_{\hat{x}}(\lambda,k) &= \Large \prod_{i=1}^{n} \frac{k}{\lambda}\left(\frac{x_i}{\lambda}\right)^{k-1}  e^{-\left(\frac{x_i}{\lambda}\right)^k} \\
 &= \Large\left(\frac{k}{\lambda}\right)^n    \left(\frac{1}{\lambda^{k-1}}\right)^n    \left(\prod_{i=1}^{n}x_i^{k-1}\right)    \left(e^{-\sum\limits_{i=1}^{n}\left(\frac{x_i}{\lambda}\right)^k}\right)  \\
 &=\Large \frac{k^n}{\lambda^{nk}}    e^{-\sum\limits_{i=1}^{n}\left(\frac{x_i}{\lambda}\right)^k}    \prod_{i=1}^{n}x_i^{k-1}    
\end{aligned}
$$


### Step 3: The log-likelihood function (ln(L)).


$$
\Large \ln L_{\hat{x}}(\lambda,k) = \Large n\ln(k) - nk\ln(\lambda) - \sum\limits_{i=1}^{n}\left(\frac{x_i}{\lambda}\right)^k + (k-1)\sum\limits_{i=1}^{n}\ln x_i
$$


### Step 4: Derivative of the log-likelihood function (ln(L)).


$$
\large \frac{d \ln L_{\hat{x}}(\lambda,k)}{d \lambda} = \Large -\frac{nk}{\lambda} + k\sum\limits_{i=1}^{n} \frac{x_i^k}{\lambda^{k+1}}
$$


```{r}
# MAYBE COMBINE STEP 4 AND 5!!!!!!!!!!!!!!
```



### Step 5: Solving for the desired parameters.


<!-- $$ -->
<!-- \begin{alignat}{2} -->
<!-- \large \frac{d \ln L_{\hat{x}}(\lambda,k)}{d \lambda} &= \Large -\frac{nk}{\lambda} + k\sum\limits_{i=1}^{n} \frac{x_i^k}{\lambda^{k+1}} &&= \Large0 \\ -->
<!--  &= \Large -\frac{nk}{\lambda} + \frac{k}{\lambda}\sum\limits_{i=1}^{n}\left(\frac{x_i}{\lambda}\right)^k && = \Large0 \\ -->
<!--  &= \Large - n +\sum\limits_{i=1}^{n}\frac{x_i^k}{\lambda^k} &&= \Large 0 \\ -->
<!--  &= \Large \frac{1}{\lambda^k} \sum\limits_{i=1}^{n}x_i^k &&= \Large n \\ -->
<!--  &= \Large \frac{1}{n} \sum\limits_{i=1}^{n}x_i^k &&= \Large \lambda^k  -->
<!-- \end{alignat} -->
<!-- $$ -->

Therefore the estimator $\hat{\lambda}$ is:

$$
\Large \hat{\lambda} = \Large \left( \frac{1}{n} \sum\limits_{i=1}^{n}x_i^k \right)^\frac{1}{k}
$$






Plugging in $\hat{\lambda}$ into the log-likelihood function (ln(L)) and then differentiating with respect to $k$:


$$
\begin{aligned}
\large \frac{d \ln L_{\hat{x}}(\lambda,k)}{d k} &= \Large \frac{d}{dk} \left[ n\ln(k) - nk\ln(\lambda) - \sum\limits_{i=1}^{n}\left(\frac{x_i}{\lambda}\right)^k + (k-1)\sum\limits_{i=1}^{n}\ln x_i \right] \\
&= x
\end{aligned}
$$






Therefore the estimator $\hat{k}$ is:




