---
title: "Project 2"
author: "Daniel Fredin, Junhan Li, & Eric Chen"
output: pdf_document
---

```{r include=FALSE, echo=FALSE}
# Libraries
library(tidyverse)
library(ggplot2)
library(dplyr)
library(splitstackshape)
library(readr)
library(car)
library(lmtest)
library(rcompanion)
library(olsrr)
library(kableExtra)
library(caret)

library(lpSolve)
library(quadprog)
```


# Introduction 

By utilizing the glass identification dataset graciously provided by the USA Forensic Science Service, we have successfully derived insights and constructed a regression equation to forecast the occurrence of silicon (Si). This prediction is based on the observed oxide content found in the various types of glass examined in the study.

# Problem 1

## Part(a)

```{r include=FALSE, echo=FALSE}
glass <- read.csv("glass.csv")
```

```{r}
# Correlation for all numerical variables
cor(glass[,c("Si", "RI", "Na", "Mg", "Al", "K", "Ca", "Ba", "Fe")])
```

```{r warning=FALSE, message=FALSE}
# Select the highest correlated variables
glass_new <- glass %>%
  select(Si, RI, type)

# Rename for facet wrap labels
glass_names <- as_labeller(
  c('Con' = "Containers",
    'Head' = "Headlamps",
    'Tabl' = "Tableware",
    'Veh' = "Vehicle Windows (float glass)",
    'WinF' = "Building Windows (float glass)",
    'WinNF' = "Building Windows (non-float glass)")
)

# Scatter plot of Si vs RI for each glass type
ggplot(data= glass_new, aes(x=RI, y= Si, color = type)) +
  geom_point() +
  geom_smooth(method = lm, se = T) +
  facet_wrap(~type, labeller = glass_names) +
  labs(title = "Comparision of Silicon with it's refractive index in different types of glass",
       x = "Refractive Index", 
       y = "Silicon",
       color = "Glass Types") +
  theme_bw() +
  theme(legend.position = "none") +
  guides(color = guide_legend(nrow = 1))
```

To visualize the relationship between two numerical variables and illustrate the variation across different types of glass, we conducted a correlation analysis. Our aim was to identify the pair of variables with the highest correlation coefficient. In accordance with the given problem, we selected silicon (Si) as our dependent variable. Subsequently, we chose the refractive index (RI) as the independent variable due to its strong correlation with Si, indicated by a coefficient of -0.54. This implies a moderately negative correlation between Si and RI. As a result, we can infer that as the silicon content decreases, the refractive index of the glass is expected to increase.

Based on our visualization, it is evident that there exists a moderately negative correlation between Si and RI across all glass types, except for container glass, which surprisingly exhibits a moderately positive correlation. This peculiar pattern may be attributed to the elevated presence of water-insoluble oxides in container glass, contributing to enhanced chemical durability against water, a crucial requirement for storing beverages and food. Another noteworthy observation is that the refractive index tends to cluster around 0 for all glass types. This indicates that light passing through the glass travels at a velocity equal to that in a vacuum, exhibiting no bending or refraction. Such consistency around the refractive index of 0 appears to be a desirable characteristic across all glass types. While the majority of glass types exhibit a RI range between -5 and 5, it is worth noting that building windows display the widest RI range, with float glass spanning from -5 to 10 and non-float glass ranging from -5 to 15. On the other hand, vehicle glass demonstrates the smallest RI range, spreading from -2 to 4. In terms of Si concentration, tableware stands out with the highest Si content compared to other glass types, whereas building windows (non-float glass) generally possess the lowest Si concentration. Additionally, tableware showcases the most significant decline in Si per unit decrease in RI, indicating a steep relationship between these variables.


```{r}
# We will first check for the normality of dependent variable
shapiro.test(glass$Si)
```
We choose the variable RI (refractive index) and Si as our x and y axis to
illustrate their correlations.  

  - 2 Paragraphs
    - why we choose variables
    - what you see from graph
    

<!-- # Part (b-e) Version 1 -->

<!-- ## Part (b) Fit at least two models(should be same type?) -->
<!-- We will use MLR -->

<!-- # Test 1 Remove some outliers -->
<!-- ```{r include=T, echo=T} -->
<!-- boxplot(glass$Si) -->
<!-- outliers <- boxplot.stats(glass$Si)$out -->
<!-- max(outliers) -->
<!-- min(outliers) -->
<!-- glass_filtered <- subset(glass, glass$Si != max(outliers)) -->
<!-- glass_filtered <- subset(glass, glass$Si != min(outliers)) -->

<!-- ``` -->

<!-- Remove all outliers(Currently our best option) -->
<!-- ```{r include=T, echo=T} -->
<!-- glass <- subset(glass, glass$Si<74) -->
<!-- glass <- subset(glass, glass$Si>71.5) -->
<!-- boxplot(glass$Si) -->
<!-- ``` -->

<!-- We will first check for the normality of dependent variable -->
<!-- ```{r include=TRUE, echo=TRUE} -->
<!-- # We will first check for the normality of dependent variable -->
<!-- shapiro.test(glass$Si) -->
<!-- ``` -->
<!-- Since the shapiro test gives us a p-value 2.175e-09, which is less than 0.05, -->
<!-- we will need to take the square root of our dependent variable to make it -->
<!-- normal. -->

<!-- # Factor data directly -->
<!-- ```{r include=TRUE, echo=TRUE} -->
<!-- # We will first check for the normality of dependent variable -->

<!-- glass_model1 <- lm(Si^3 ~. , data = glass) -->
<!-- shapiro.test(residuals(glass_model1)) -->

<!-- glass_model2 <- lm(Si^3  ~ RI + factor(type) + Na , data = glass) -->
<!-- shapiro.test(residuals(glass_model2)) -->


<!-- mutate_glass <- glass %>% -->
<!--   mutate(type= case_when( -->
<!--     type %in% ("Head") ~ "Head", -->
<!--     TRUE ~ "All Other types")) -->

<!-- glass_model3 <- lm(Si^3 ~. -RI - Mg - Ca - type  , data = mutate_glass) -->
<!-- shapiro.test(residuals(glass_model3)) -->

<!-- ``` -->
<!-- After we have take the square root of the dpendent variable Si, we can -->


<!-- - select the best competing model for statistical analyses -->
<!-- ```{r include=T, echo=T} -->
<!-- summary(glass_model1) -->
<!-- summary(glass_model2) -->
<!-- summary(glass_model3) -->
<!-- #. AIC -->

<!-- compareLM(glass_model1, glass_model2, glass_model3) -->
<!-- ``` -->

```{r}
boxplot(glass$Si)
```


## Part (b) 

```{r} 
set.seed(123)

# Regular linear regression with all variables
glass_model1 <- lm(Si ~., data = glass)
predictions <- glass_model1 %>% predict(glass)
model1_results <- data.frame( R2 = R2(predictions, glass$Si),
            RMSE = RMSE(predictions, glass$Si),
            MAE = MAE(predictions, glass$Si))
model1_results



# Regular linear regression with 4 variables from our visualization
glass_model2 <- lm(Si  ~ RI + factor(type) + Na + Ca , data = glass)
predictions <- glass_model2 %>% predict(glass)
model2_results <- data.frame( R2 = R2(predictions, glass$Si),
             RMSE = RMSE(predictions, glass$Si),
             MAE = MAE(predictions, glass$Si))
model2_results

# Training and testing method (SEEMS LIKE OUR BEST MODEL BECAUSE OF LOWER RMSE)
training <- glass$Si %>%
    createDataPartition(p = .75, list = FALSE)
train.data  <- glass[training, ]
test.data <- glass[-training, ]

glass_model3 <- lm(Si ~ Na + Mg + Al + K + Ca + Ba + Fe, data = train.data)
predictions <- glass_model3 %>% predict(test.data)
model3_results <- data.frame( R2 = R2(predictions, test.data$Si),
            RMSE = RMSE(predictions, test.data$Si),
            MAE = MAE(predictions, test.data$Si))
model3_results

# varImp(glass_model3)




# # model 4 will use k-cross validation
# ctrl <- trainControl(method = "cv", number = 10)
# glass_model4 <- train(Si ~ ., data = glass, method = "lm", trControl = ctrl)
# print(glass_model4)
#  
# # model 5 will use repeated k-cross validation
# ctrl_repeat <- trainControl(method = "repeatedcv", number = 10,  repeats = 3)
# glass_model5 <- train(Si ~ ., data = glass, 
#                        method = "lm", trControl = ctrl_repeat)
# print(glass_model5)


```

Argue your choice
- Perform optimization in R, compute the regression coefficients

The lower the value for AIC, the better the fit of the model. The absolute value of the AIC value is not important. It can be positive or negative.

Preconlcusion: Model 3 is a lot better. 
Based on the RMSE values we generated for the three models above, the model 3
where we used 75% training data model is our best model


## Part (c) 

* (i) Coefficient of determination

```{r}
RMSE <- model3_results[1]
RMSE
```

The coefficient of determination, also known as R-squared, is a metric used in regression analysis to evaluate how well a model fits the data. It indicates the proportion of the dependent variable's variability that can be explained by the independent variables. R-squared ranges from 0 to 1, where 0 signifies that the independent variables have no influence on the variation, and 1 means they explain all of it. Our best model had an impressive R-squared value of 0.9930, indicating that 99.30% of the dependent variable's variance is captured by the independent variables.



* (ii) Least-squares estimates for the regression line

```{r}
summary(glass_model3)
```

Regression equation:

$$
\begin{aligned}
\Large Si &= \Large  98.81 - (0.94 \cdot Na) - (0.99 \cdot Mg) - (1.00 \cdot Al) \\ &- \Large (0.98 \cdot K) - (0.98 \cdot Ca) - (0.96 \cdot Ba) - (0.35 \cdot Fe)
\end{aligned}
$$

* (iii) Slopes and intercept interpretation


  - determine the significant predictors at 10% level of significance



All of the variables in our best model are significant predictors of Si content at 10% confidence. This regression equation suggests that when none of the critical elements are present in the glass, 98.81% of the material will be Si The negative slopes in the equation means that an increase of the presence of any other element by one percent will decrease the Si content of the glass by roughly one percent, which makes sense. However, the exception is Fe, which only decreases Si content by 0.35% for every one percent change.


* (iv) Residual analysis 

  - the four tests
  - four plots group together
  
```{r}
par(mfrow = c(2,2))
plot(glass_model3)

# Check for Linearity of glass_model3 using rainbow test
raintest(glass_model3)



# Check for normality of glass_model3 using shapiro test on residuals
# (This predicts residuals on training data)
real_resid <- residuals(glass_model3)
shapiro.test(real_resid) # violated 

# (This predicts residuals on TESTING data)
resids <- test.data$Si - predictions # This is what we want!
shapiro.test(resids) 

# Check for multicollinearity 
vif(glass_model3)
 
dwtest(glass_model3)

ols_test_breusch_pagan(glass_model3) # violated

ols_plot_resid_stud(glass_model3) # OUTLIER!!!!
```

Based on the rainbow test on our model, the linearity assumption is not violated, as the null hypothesis of a linear fit is not rejected (p > 0.05). However, the results of a Shapiro test on the residuals of the model conclude they are not normal, as p < 0.05 here. Heteroscedasticity is also present in the residuals, as is an outlier. Thus, the assumptions on residuals of regression models to be normal do not hold.


## Part (d)

```{r}

glass_mutate <- glass %>%
  filter(type == "WinF" | type == "WinNF")

# Check 1 Independent of one another
boxplot(Al ~ type, 
        data = glass_mutate, 
        main = "Comparison of AL and types: WinF and WinNF")

# Check 2: Normality
shapiro.test(glass_mutate$Al)


# Check 3: Levene's test, The two groups have equal variance
var.test(Al ~ type, 
         data = glass_mutate,
         conf.level = 0.9)


# Perform a non-parametric test: Since the distribution of Al is not normal, use a 
# non-parametric test, such as the Wilcoxon-Mann-Whitney test, to compare the 
# medians of the two groups
wilcox_result <- wilcox.test(Al ~ type, 
                             data = glass_mutate, 
                             var.equal = TRUE,
                             conf.level = 0.9)
wilcox_result
p_value <- wilcox_result$p.value

```

```{r include = TRUE, echo=FALSE}
if (p_value < 0.1) {
  cat("\nThe distributions of the two groups are significantly different at the 10% significance level.")
} else {
  cat("\nThe distributions of the two groups are not significantly different at the 10% significance level.")
}
```


The Shaprio test reveals a deviation from normal distribution in Al, as evidenced by a p-value below 0.05. Consequently, a t-test comparing the means of the two groups cannot be conducted due to the non-normality. However, the two groups are independent and exhibit variances that are not significantly dissimilar, as indicated by an F test p-value exceeding 0.05. Thus, it is appropriate to employ a non-parametric test like the Wilcoxon-Mann-Whitney test to compare the medians of the two groups. Employing a 10% level of significance, the distributions of the two groups, WinF and WinNF, are markedly distinct since the p-value for the Wilcoxon test is significantly below 0.05.



## Part (e):

```{r}
set.seed(123)
# Need 214 values
N <- 214

Na <- rnorm(n=N, mean = mean(glass$Na), sd(glass$Na))
Mg <- rnorm(n=N, mean = mean(glass$Mg), sd(glass$Mg))
Al <- rnorm(n=N, mean = mean(glass$Al), sd(glass$Al))
K <- rnorm(n=N, mean = mean(glass$K), sd(glass$K))
Ca <- rnorm(n=N, mean = mean(glass$Ca), sd(glass$Ca))
Ba <- rnorm(n=N, mean = mean(glass$Ba), sd(glass$Ba))
Fe <- rnorm(n=N, mean = mean(glass$Fe), sd(glass$Fe))

e <- rnorm(n=N, 0, 1)

# Saves coefficients from our model3 as variables
coef <- summary(glass_model3)$coefficients

y <- e + coef[1] + coef[2]*Na + coef[3]*Mg + coef[4]*Al + coef[5]*K + 
  coef[6]*Ca + coef[7]*Ba + coef[8]*Fe

summary(y)

```




```{r}
# Simulated model
sim_model <- lm(y ~ Na + Mg + Al + K + Ca + Ba + Fe)
summary(sim_model)
```
Simulated Model Regression equation:

$$
\begin{aligned}
\Large Si &= \Large  98.32 - (0.89 \cdot Na) - (0.989 \cdot Mg) - (0.993 \cdot Al) \\ &- \Large (1.26 \cdot K) - (0.98 \cdot Ca) - (0.805 \cdot Ba) + (1.05 \cdot Fe)
\end{aligned}
$$

This regression equation suggests that when none of the critical elements are present in the glass, 98.32% of the material will be Si. The negative slopes in the equation means that an increase of the presence of any other element by one percent will decrease the Si content of the glass by roughly one percent, which makes sense. 

It is clearly to see that our simulated regression has completely different coefficients
for iron where for every one percent of increase in iron, the percentage of materials
that are silicon in the glass will increases by 1.05%. (Certain chemical properties?)

```{r}
# Check for Linearity of glass_model3 using rainbow test
raintest(sim_model)

# Check for normality of glass_model3 using shapiro test on residuals
sim_resid <- residuals(sim_model)

shapiro.test(sim_resid) # violated

# Check for multicollinearity
vif(sim_model)
 
dwtest(sim_model)

ols_test_breusch_pagan(sim_model) # violated

ols_plot_resid_stud(sim_model) # OUTLIER!!!!
```









# Problem 3

## Part (a)
```{r include=FALSE, echo=FALSE}
Handout1 <- read.csv("Handout 1.csv")
```

We opted to select independent variables that exclusively comprised discrete or continuous numerical values, excluding factors such as the teacher's sex, type of school, or teaching level. Our decision was based on the belief that this variable selection, combined with 10-fold cross-validation, would yield the highest level of accuracy in our model.

### 10-fold Cross-Validation

```{r}
# Convert COMMIT to binary < median = 0, otherwise = 1.

new_Handout1 <- Handout1 %>%
  mutate(COMMIT = case_when(
    (COMMIT < median(COMMIT)) ~ 0,
    TRUE ~ 1))

# Select only the continuous numerical variables
new_Handout1 <- new_Handout1 %>%
  select(COMMIT, AGE, SALARY, CLASSSIZE, RESOURCES, AUTONOMY, CLIMATE, SUPPORT)


# Training and testing method
set.seed(123)
# Create training and testing data
index <- createDataPartition(new_Handout1$COMMIT, 
                             p = 0.8, 
                             list = FALSE,
                             times=1)


new_Handout1 <- as.data.frame(new_Handout1)

train  <- new_Handout1[index, ]
test <- new_Handout1[-index, ]

train$COMMIT[train$COMMIT==1] <- "yes"
train$COMMIT[train$COMMIT==0] <- "no"

test$COMMIT[test$COMMIT==1] <- "yes"
test$COMMIT[test$COMMIT==0] <- "no"


# Convert outcome variable to factor for each data frame
train$COMMIT <- as.factor(train$COMMIT)
test$COMMIT <- as.factor(test$COMMIT)

# 10-fold Cross-validation method
ctrlspecs <- trainControl(method="cv", 
                          number=10, 
                          savePredictions="all",
                          classProbs=TRUE)

set.seed(1234)
cvmodel <- train(COMMIT ~ .,  
                 data=train, 
                 method="glm", 
                 family = "binomial",  
                 trControl=ctrlspecs)

# Model summary for 10-fold Cross-validation method
summary(cvmodel)
# Model for 10-fold Cross-validation method
print(cvmodel)
# Important variables for 10-fold Cross-validation method
varImp(cvmodel)
```

### Model Accuracy

```{r}
# Predict outcome using model from training data based on testing data
predictions <- predict(cvmodel, newdata=test)

# Create confusion matrix to assess model fit/performance on test data
con_matx <- confusionMatrix(data=predictions, test$COMMIT)
con_matx
```

After examining the confusion matrix, it becomes apparent that our model attained an accuracy rate of 80% (95% CI: 0.61-0.92) when tested on the data. As the p-value exceeds 0.05, this suggests that our model exhibits a statistically significant level of accuracy.

## Part (b)

Our objective is to optimize the cost function in order to identify the most economical approach for fulfilling the Christmas order. This entails striving for the lowest cost per day to achieve maximum cost efficiency. By minimizing expenses at each factory, we can ensure a cost-effective process for meeting the demands of the Christmas order.


```{r}
factories <- c("Factory A", "Factory B", "Factory C")
toys <- c("Cars", "Animals", "Robots")

# Define the coefficients of the objective function
costs <- c(1000, 2100, 1500)

# Define the constraint matrix
constraint_matrix <- matrix(c(30, 20, 30,
                              40, 50, 10,
                              50, 40, 15), nrow = 3, byrow = TRUE)

# Define the right-hand side of the constraints
constraint_limits <- c(5000, 3000, 2500)

# Set the direction of optimization (minimization)
constraint_directions <- c(">=", ">=", ">=")
direction <- "min"

# Solve the linear programming problem
min_solution <- lp(direction = direction,
                   objective.in = costs,
                   const.mat = constraint_matrix,
                   const.dir = constraint_directions,
                   const.rhs = constraint_limits, 
                   compute.sens=TRUE)

# Check if a solution was found
if (min_solution$status == 0) {
  # Print the optimal solution
  cat("The optimal solution is:\n")
  cat("Factory A:",min_solution$solution[1], "days","\n")
  cat("Factory B:",min_solution$solution[2], "days","\n")
  cat("Factory C:",min_solution$solution[3], "days","\n\n")
  # Print the minimum cost
  cat("The value of the objective function at the optimal solution is:\n")
  cat("Minimum cost: $",min_solution$objval)
} else {
  # No feasible solution found
  print("No feasible solution found.")
}
```

The most efficient approach entails running Factory A for approximately 166.67 days, resulting in a minimal cost of $166,666.70, whereas Factory B and Factory C remain non-operational. By adopting this strategy, the company can achieve the most cost-effective outcome.

When considering the optimal solution, it is important to analyze the cost implications of running the factories for different durations. In this scenario, operating Factory for approximately 166.67 days leads to the minimum overall cost. By halting the operations of Factory B and Factory C, the company can avoid additional expenses associated with their functioning.


```{r include=TRUE, echo=FALSE}
# Create an empty data frame to store the results
result_df <- data.frame()
factories <- c("Factory A", "Factory B", "Factory C")
toys <- c("Cars", "Animals", "Robots")

# Iterate over the range of 1:3
for (i in 1:3) {
  
  # Store values for rows and columns in the data frame
  row_df <- data.frame(Factory = factories[i],
                       min_days = min_solution$solution[i],
                       min_cost = min_solution$solution[i]*costs[i])
  result_df <- rbind(result_df, row_df)

}

colnames(result_df) <- c("",
                         "Minimum operating days",
                         "Minimum costs")

# Print the resulting data frame
cat("The summary of the results:")
print(result_df)
```


```{r}
# Sensitivity Analysis

min_solution$sens.coef.from
min_solution$sens.coef.to

```


## Part (c)

```{r}
# Weibull statistics
# lambda is scale while K is shape
compute_weibull_stats <- function(shape, scale) {
  
  # Compute the mean
  mean_value <- scale * gamma(1 + (1/shape))
  
  # Compute the median
  median_value <- scale * (log(2)^(1/shape))
  
  # Compute the mode
  if (shape > 1)
    mode_value <- scale * ((shape - 1) / shape)^(1/shape)
  else
    mode_value <- 0
  
  # Compute the variance
  variance_value <- scale^2 * (gamma(1 + (2/shape)) - (gamma(1 + (1/shape)))^2)
  
  # Return the computed statistics as a named list
  return(list(mean = mean_value, 
              median = median_value, 
              mode = mode_value, 
              variance = variance_value))
}

# Example usage
scale_param <- 6
shape_param <- 1

stats <- compute_weibull_stats(shape_param, scale_param)

# Accessing the computed statistics
mean_value <- stats$mean
median_value <- stats$median
mode_value <- stats$mode
variance_value <- stats$variance
```

When $\lambda =`r scale_param`$ and $k = `r shape_param`$ then the Weibull statistics are:  
$Mean = `r mean_value`\\$
$Median = `r median_value` \\$
$Mode = `r mode_value` \\$
$Variance = `r variance_value`$


### Maximum likelihood estimator (MLE) of the parameters from the Weibull distribution


Probability Density Function $(f(x;\lambda,k))$ for the Weibull distribution:


$$
\Large f(x;\lambda,k) = 
\begin{cases}
  \Large\frac{k}{\lambda}\left(\frac{x}{\lambda}\right)^{k-1}  e^{-\left(\frac{x}{\lambda}\right)^k}  , & x \ge 0\\
  \Large0, &x < 0
\end{cases}
$$


The likelihood function $(L_{\hat{x}}(\lambda,k))$:


$$
\begin{aligned}
 \Large L_{\hat{x}}(\lambda,k) &= \Large \prod_{i=1}^{n} \frac{k}{\lambda}\left(\frac{x_i}{\lambda}\right)^{k-1}  e^{-\left(\frac{x_i}{\lambda}\right)^k} \\
 &= \Large\left(\frac{k}{\lambda}\right)^n    \left(\frac{1}{\lambda^{k-1}}\right)^n    \left(\prod_{i=1}^{n}x_i^{k-1}\right)    \left(e^{-\sum\limits_{i=1}^{n}\left(\frac{x_i}{\lambda}\right)^k}\right)  \\
 &=\Large \frac{k^n}{\lambda^{nk}}    e^{-\sum\limits_{i=1}^{n}\left(\frac{x_i}{\lambda}\right)^k}    \prod_{i=1}^{n}x_i^{k-1}    
\end{aligned}
$$


The log-likelihood function $(\ln L_{\hat{x}}(\lambda,k))$:


$$
\Large \ln L_{\hat{x}}(\lambda,k) = \Large n\ln(k) - nk\ln(\lambda) - \sum\limits_{i=1}^{n}\left(\frac{x_i}{\lambda}\right)^k + (k-1)\sum\limits_{i=1}^{n}\ln x_i
$$


Partial derivative of the log-likelihood function with respect to $\lambda$:


$$
\large \frac{\partial \ln L_{\hat{x}}(\lambda,k)}{\partial \lambda} = \Large -\frac{nk}{\lambda} + k\sum\limits_{i=1}^{n} \frac{x_i^k}{\lambda^{k+1}}
$$


Solving for the desired parameter of $\lambda$ by setting the partial derivative equal to zero:

$$
\begin{aligned}
\large \frac{\partial \ln L_{\hat{x}}(\lambda,k)}{\partial \lambda} &= \Large 0 \\
\Large -\frac{nk}{\lambda} + k\sum\limits_{i=1}^{n} \frac{x_i^k}{\lambda^{k+1}} &= \Large0 \\
\Large -\frac{nk}{\lambda} + \frac{k}{\lambda}\sum\limits_{i=1}^{n}\left(\frac{x_i}{\lambda}\right)^k & = \Large0 \\
\Large - n +\sum\limits_{i=1}^{n}\frac{x_i^k}{\lambda^k} &= \Large 0 \\
\Large \frac{1}{\lambda^k} \sum\limits_{i=1}^{n}x_i^k &= \Large n \\
\Large \frac{1}{n} \sum\limits_{i=1}^{n}x_i^k &= \Large \lambda^k
\end{aligned}
$$

Therefore the estimator $\hat{\lambda}$ is:

$$
\Large \hat{\lambda} = \Large \left( \frac{1}{n} \sum\limits_{i=1}^{n}x_i^k \right)^\frac{1}{k}
$$






Plugging in $\hat{\lambda}$ into the log-likelihood function $(\ln L_{\hat{x}}(\lambda,k))$ and then differentiating with respect to $k$ in order to find the estimator $\hat{k}$:


$$
\begin{aligned}
\large \frac{\partial \ln L_{\hat{x}}(\lambda,k)}{\partial k} &= \Large \frac{\partial}{\partial k} \left[ n\ln k - nk\ln \lambda - \sum\limits_{i=1}^{n}\left(\frac{x_i}{\lambda}\right)^k + (k-1)\sum\limits_{i=1}^{n}\ln x_i \right] \\
&= \Large \frac{\partial}{\partial k} \left[ n\ln k - nk\ln\left[ \left( \frac{1}{n} \sum\limits_{i=1}^{n}x_i^k \right)^\frac{1}{k}  \right] - \frac{\sum\limits_{i=1}^{n}x_i^k}{\left[ \left( \frac{1}{n} \sum\limits_{i=1}^{n}x_i^k \right)^\frac{1}{k} \right]^k} + (k-1)\sum\limits_{i=1}^{n}\ln x_i \right] \\
&= \Large \frac{\partial}{\partial k} \left[ n\ln k - \frac{nk}{k}\ln\left( \frac{1}{n} \sum\limits_{i=1}^{n}x_i^k \right) - \frac{\sum\limits_{i=1}^{n}x_i^k}{\left( \frac{1}{n} \sum\limits_{i=1}^{n}x_i^k \right)} + (k-1)\sum\limits_{i=1}^{n}\ln x_i \right] \\
&= \Large \frac{\partial}{\partial k} \left[ n\ln k - n\ln\left( \frac{1}{n} \sum\limits_{i=1}^{n}x_i^k \right) - n + (k-1)\sum\limits_{i=1}^{n}\ln x_i \right] \\
&= \Large \frac{n}{k} - \left( \frac{n\sum\limits_{i=1}^{n}x_i^k\ln{x_i}}{\sum\limits_{i=1}^{n}x_i^k} \right) + \sum\limits_{i=1}^{n}\ln x_i \\
&= \Large \frac{1}{k} - \left( \frac{\sum\limits_{i=1}^{n}x_i^k\ln{x_i}}{\sum\limits_{i=1}^{n}x_i^k} \right) + \frac{1}{n}\sum\limits_{i=1}^{n}\ln x_i
\end{aligned}
$$


Solving for the desired parameter of $k$ by setting the partial derivative equal to zero:

$$
\begin{aligned}
\large \frac{\partial \ln L_{\hat{x}}(\lambda,k)}{\partial k} &= \Large 0 \\
\Large \frac{1}{k} - \left( \frac{\sum\limits_{i=1}^{n}x_i^k\ln{x_i}}{\sum\limits_{i=1}^{n}x_i^k} \right) + \frac{1}{n}\sum\limits_{i=1}^{n}\ln x_i &= \Large 0 \\
\Large  \left( \frac{\sum\limits_{i=1}^{n}x_i^k\ln{x_i}}{\sum\limits_{i=1}^{n}x_i^k} \right) - \frac{1}{n}\sum\limits_{i=1}^{n}\ln x_i&= \Large \frac{1}{k}
\end{aligned}
$$



Therefore the estimator $\hat{k}$ is:

$$
\Large \hat{k} = \left[ \Large \frac{\sum\limits_{i=1}^{n}x_i^k\ln{x_i}}{\sum\limits_{i=1}^{n}x_i^k}  - \frac{1}{n}\sum\limits_{i=1}^{n}\ln x_i \right]^{-1}
$$

The definition of $\hat{k}$ provided here is implicit, as determining the value of $k$ typically requires numerical methods for solution.