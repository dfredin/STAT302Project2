---
title: "Project 2"
author: "Daniel Fredin, Junhan Li, & Eric Chen"
output: pdf_document
---

```{r include=FALSE, echo=FALSE}
# Libraries
library(tidyverse)
library(ggplot2)
library(dplyr)
library(splitstackshape)
library(readr)
library(car)
library(lmtest)
library(rcompanion)
library(olsrr)
library(kableExtra)
library(caret)

library(lpSolve)
library(quadprog)
```




# Problem 1

## Testing 
```{r include=T, echo=T}
glass <- read.csv("glass.csv")
```


- response variable will be (Si)
```{r}
cor(glass[,c("Si", "RI", "Na", "Mg", "Al", "K", "Ca", "Ba", "Fe")])
```


# part(a)
- Select four variables of my choice
```{r include=True, echo=FALSE}
glass_new <- glass %>%
  # COMMIT,AGE, SALARY, CLIMATE, SUPPORT
  select(Si, RI, Ca, type)
head(glass_new)
```
- use visualization plot to tell a story, making use the variables selected
- Using color,faceting, theme in ggplot2
```{r include=TRUE, echo=TRUE}
ggplot(data= glass_new, aes(x=RI, y= Si, color = type)) +
  geom_point()+
  facet_wrap(~type)
```


```{r include=TRUE, echo=TRUE}
# We will first check for the normality of dependent variable
shapiro.test(glass$Si)
```


  - 2 Paragraphs
    - why we choose variables
    - what you see from graph
    
 Initial assumptions
  - Dependent variable should be approximately normally distributed
  - Variables should related to each other linear - use a matrix scatterplot
  - There should be no significant outliers - Kept at minimum   
  
  
  
  
# Part (b-e) Version 1

## Part (b) Fit at least two models(should be same type?)
We will use MLR

# Test 1 Remove some outliers
```{r include=T, echo=T}
boxplot(glass$Si)
outliers <- boxplot.stats(glass$Si)$out
max(outliers)
min(outliers)
glass_filtered <- subset(glass, glass$Si != max(outliers))
glass_filtered <- subset(glass, glass$Si != min(outliers))

```

Remove all outliers(Currently our best option)
```{r include=T, echo=T}
glass <- subset(glass, glass$Si<74)
glass <- subset(glass, glass$Si>71.5)
boxplot(glass$Si)
```

We will first check for the normality of dependent variable
```{r include=TRUE, echo=TRUE}
# We will first check for the normality of dependent variable
shapiro.test(glass$Si)
```
Since the shapiro test gives us a p-value 2.175e-09, which is less than 0.05,
we will need to take the square root of our dependent variable to make it
normal. 

# Factor data directly
```{r include=TRUE, echo=TRUE}
# We will first check for the normality of dependent variable

glass_model1 <- lm(Si^3 ~. , data = glass)
shapiro.test(residuals(glass_model1))

glass_model2 <- lm(Si^3  ~ RI + factor(type) + Na , data = glass)
shapiro.test(residuals(glass_model2))


mutate_glass <- glass %>%
  mutate(type= case_when(
    type %in% ("Head") ~ "Head",
    TRUE ~ "All Other types"))

glass_model3 <- lm(Si^3 ~. -RI - Mg - Ca - type  , data = mutate_glass)
shapiro.test(residuals(glass_model3))

```
After we have take the square root of the dpendent variable Si, we can


- select the best competing model for statistical analyses
```{r include=T, echo=T}
summary(glass_model1)
summary(glass_model2)
summary(glass_model3)
#. AIC

compareLM(glass_model1, glass_model2, glass_model3)
```





## Part (b) version 2
# (b) Training and testing / k cross-validation method
```{r include=T, echo=T} 
set.seed(123)
# Regular linear regression with all variables
glass_model1 <- lm(Si ~., data = glass)
predictions <- glass_model1 %>% predict(glass)
results <- data.frame( R2 = R2(predictions, glass$Si),
            RMSE = RMSE(predictions, glass$Si),
            MAE = MAE(predictions, glass$Si))
results

# Regular linear regression with 3 variables
glass_model2 <- lm(Si  ~ RI + factor(type) + Na , data = glass)
predictions <- glass_model2 %>% predict(glass)
results <- data.frame( R2 = R2(predictions, glass$Si),
            RMSE = RMSE(predictions, glass$Si),
            MAE = MAE(predictions, glass$Si))
results

# Training and testing method (SEEMS LIKE OUR BEST MODEL)
training <- glass$Si %>%
    createDataPartition(p = .75, list = FALSE)
train.data  <- glass[training, ]
test.data <- glass[-training, ]

glass_model3 <- lm(Si ~., data = train.data)
predictions <- glass_model3 %>% predict(test.data)
results <- data.frame( R2 = R2(predictions, test.data$Si),
            RMSE = RMSE(predictions, test.data$Si),
            MAE = MAE(predictions, test.data$Si))
results


# varImp(glass_model3)


# model 4 will use k-cross validation
ctrl <- trainControl(method = "cv", number = 10)
glass_model4 <- train(Si ~ ., data = glass, method = "lm", trControl = ctrl)
print(glass_model4)



# model 5 will use repeated k-cross validation
ctrl_repeat <- trainControl(method = "repeatedcv", number = 10,  repeats = 3)
glass_model5 <- train(Si ~ ., data = glass, 
                      method = "lm", trControl = ctrl_repeat)
print(glass_model5)


```

Argue your choice
- Perform optimization in R, compute the regression coefficients

The lower the value for AIC, the better the fit of the model. The absolute value of the AIC value is not important. It can be positive or negative.

Preconlcusion: Model 2 is a lost better. 
Based on the RMSE values we generated for the four mdoels above, the model 3
where we used 70% training data model is our best model


# Part (c) Use result of best model, answer following questions
(i): Explain coefficient of determination
```{r}
results[1]
```


(ii): Find Least-squares estimates: Basically Regression
```{r}
summary(glass_model3)
```

(iii): Interpret slopes and intercept of the problem
  - determine the significant predictors at 10% level of significance

$$
\begin{aligned}
S_i = &-0.576 - (0.034 \cdot Age) + (0.068 \cdot NumberInHousehold) + (0.534 \cdot All Other Marital Statuses)\\ &+ (1.049 \cdot Divorced Or Widowed) + (0.857 \cdot Married / remarried) + (2.163 \cdot Separated) \\ &- (1.457 \times 10^{-5} \cdot SavingsBalance) - (3.520 \times 10^{-5} \cdot CheckingBalance) \\ &- (0.320 \cdot All Other Genders) + (0.383 \cdot Female) - (0.071 \cdot All OtherStates) \\ &- (0.102 \cdot African American) + (0.097 \cdot All Other Ethinicities)
\end{aligned}
$$
  
(iv): Perform Residual analysis 
  - the four tests
  - four plots group together
  
```{r}
par(mfrow = c(2,2))
plot(glass_model3)

# Check for Linearity of glass_model3 using rainbow test
raintest(glass_model3)

# Check for normality of glass_model3 using shaprio tes on residuals
lr_reid <- residuals(glass_model3)

shapiro.test(lr_reid) # violated

vif(glass_model3) # violated multicollinearity
 
dwtest(glass_model3)
 
raintest(glass_model3)

ols_test_breusch_pagan(glass_model3) # violated
```
# Fix
``` {R}
# Training and testing method (SEEMS LIKE OUR BEST MODEL)
training <- glass$Si %>%
    createDataPartition(p = .75, list = FALSE)
train.data  <- glass[training, ]
test.data <- glass[-training, ]

glass_model3 <- lm(Si ~. - Mg - Ca - type, data = train.data)
predictions <- glass_model3 %>% predict(test.data)
results <- data.frame( R2 = R2(predictions, test.data$Si),
            RMSE = RMSE(predictions, test.data$Si),
            MAE = MAE(predictions, test.data$Si))
results


lr_reid <- residuals(glass_model3)

shapiro.test(lr_reid) 
```



# Part (d): separate observations into 
- group 1: type = WinF
- group 2: type= WonNF
Variable A1, level alpha = 0.1 


```{r}
# Assuming your dataset is named 'data' and the variable of interest is 'Al'
group1 <- subset(glass, type == "WinF")$Al
group2 <- subset(glass, type == "WinNF")$Al

# Perform a t-test: If the distribution of the variable Al follows a normal distribution, use a t-test to compare the means of the two groups
ttest_result <- t.test(group1, group2)
ttest_result

p_value <- ttest_result$p.value

# Interpret the t-test results
if (p_value < 0.1) {
  cat("The distributions of the two groups are significantly different at the 0.10 significance level.")
} else {
  cat("The distributions of the two groups are not significantly different at the 0.10 significance level.")
}

```

```{r}
# Perform a non-parametric test: If the distribution of Al is not normal, use a non-parametric test, such as the Wilcoxon-Mann-Whitney test, to compare the medians of the two groups

shapiro.test(glass$Al)


wilcox_result <- wilcox.test(group1, group2)
wilcox_result
p_value <- wilcox_result$p.value

if (p_value < 0.1) {
  cat("The distributions of the two groups are significantly different at the 0.10 significance level.")
} else {
  cat("The distributions of the two groups are not significantly different at the 0.10 significance level.")
}

```


# Part (e):









# Problem 3

## a)
```{r include=FALSE, echo=FALSE}
Handout1 <- read.csv("Handout 1.csv")
```

### i)
We choose to use 10-cross validation. Why we choose it 

### ii) 10-fold Cross-Validation

```{r include=FALSE, echo=FALSE}
# Convert COMMIT to binary < median = 0, otherwise = 1.

new_Handout1 <- Handout1 %>%
  mutate(COMMIT = case_when(
    (COMMIT < median(COMMIT)) ~ 0,
    TRUE ~ 1))

# Select only the continuous numerical variables
new_Handout1 <- new_Handout1 %>%
  select(COMMIT, AGE, SALARY, CLASSSIZE, RESOURCES, AUTONOMY, CLIMATE, SUPPORT)


# Training and testing method
set.seed(123)
# Create training and testing data
index <- createDataPartition(new_Handout1$COMMIT, 
                             p = 0.8, 
                             list = FALSE,
                             times=1)


new_Handout1 <- as.data.frame(new_Handout1)

train  <- new_Handout1[index, ]
test <- new_Handout1[-index, ]

train$COMMIT[train$COMMIT==1] <- "yes"
train$COMMIT[train$COMMIT==0] <- "no"

test$COMMIT[test$COMMIT==1] <- "yes"
test$COMMIT[test$COMMIT==0] <- "no"


# Convert outcome variable to factor for each data frame
train$COMMIT <- as.factor(train$COMMIT)
test$COMMIT <- as.factor(test$COMMIT)

# 10-fold Cross-validation method
ctrlspecs <- trainControl(method="cv", 
                          number=10, 
                          savePredictions="all",
                          classProbs=TRUE)

set.seed(1234)
cvmodel <- train(COMMIT ~ .,  
                 data=train, 
                 method="glm", 
                 family = "binomial",  
                 trControl=ctrlspecs)


# results1 <- data.frame( R2 = R2(predictions, test$COMMIT),
#             RMSE = RMSE(predictions, test$COMMIT),
#             MAE = MAE(predictions, test$COMMIT))
# # Results for 10-fold Cross-validation method
# results1

# Model for 10-fold Cross-validation method
summary(cvmodel)
print(cvmodel)
varImp(cvmodel)

```

### Model Accuracy

```{r}
# Predict outcome using model from training data based on testing data
predictions <- predict(cvmodel, newdata=test)

# Create confusion matrix to assess model fit/performance on test data
con_matx <- confusionMatrix(data=predictions, test$COMMIT)
con_matx

```


## b)

Our objective is to optimize the cost function in order to identify the most economical approach for fulfilling the Christmas order. This entails striving for the lowest cost per day to achieve maximum cost efficiency. By minimizing expenses at each factory, we can ensure a cost-effective process for meeting the demands of the Christmas order.


```{r}
factories <- c("Factory A", "Factory B", "Factory C")
toys <- c("Cars", "Animals", "Robots")

# Define the coefficients of the objective function
costs <- c(1000, 2100, 1500)

# Define the constraint matrix
constraint_matrix <- matrix(c(30, 20, 30,
                              40, 50, 10,
                              50, 40, 15), nrow = 3, byrow = TRUE)

# Define the right-hand side of the constraints
constraint_limits <- c(5000, 3000, 2500)

# Set the direction of optimization (minimization)
constraint_directions <- c(">=", ">=", ">=")
direction <- "min"

# Solve the linear programming problem
min_solution <- lp(direction = direction,
               objective.in = costs,
               const.mat = constraint_matrix,
               const.dir = constraint_directions,
               const.rhs = constraint_limits, 
               compute.sens=TRUE)


# Check if a solution was found
if (min_solution$status == 0) {
  # Print the optimal solution
  cat("The optimal solution is:\n")
  cat("Factory A:",min_solution$solution[1], "days","\n")
  cat("Factory B:",min_solution$solution[2], "days","\n")
  cat("Factory C:",min_solution$solution[3], "days","\n\n")
  # Print the minimum cost
  cat("The value of the objective function at the optimal solution is:\n")
  cat("Minimum cost: $",min_solution$objval)
} else {
  # No feasible solution found
  print("No feasible solution found.")
}
```

The most efficient approach entails running Factory A for approximately 166.67 days, resulting in a minimal cost of $166,666.70, whereas Factory B and Factory C remain non-operational. By adopting this strategy, the company can achieve the most cost-effective outcome.

When considering the optimal solution, it is important to analyze the cost implications of running the factories for different durations. In this scenario, operating Factory for approximately 166.67 days leads to the minimum overall cost. By halting the operations of Factory B and Factory C, the company can avoid additional expenses associated with their functioning.


```{r}
# Create an empty data frame to store the results
result_df <- data.frame()
factories <- c("Factory A", "Factory B", "Factory C")
toys <- c("Cars", "Animals", "Robots")



# Iterate over the range of 1:3
for (i in 1:3) {
  
  # Store values for rows and columns in the data frame
  row_df <- data.frame(Factory = factories[i],
                       min_days = min_solution$solution[i],
                       min_cost = min_solution$solution[i]*costs[i])

  
  result_df <- rbind(result_df, row_df)

}

colnames(result_df) <- c("",
                         "Minimum # of days",
                         "Minimum costs")

# Print the resulting data frame
cat("The optimal solution is:")
print(result_df)


```


```{r, include=FALSE, echo=FALSE}
# # Create an empty data frame to store the results
# result_df <- data.frame()
# factories <- c("Factory A", "Factory B", "Factory C")
# toys <- c("Cars", "Animals", "Robots")
# 
# 
# 
# # Iterate over the range of 1:3
# for (i in 1:3) {
# 
#   # Store values for rows and columns in the data frame
#   row_df <- data.frame(Factory = factories[i],
#                        cost_per_day = costs[i])
# 
# 
#   result_df <- rbind(result_df, row_df)
# 
# }
# result_df <- cbind(result_df, constraint_matrix)
# 
# 
# 
# result_df[,ncol(result_df)+ 1] <- c(min_solution$solution[1],
#                                     min_solution$solution[2],
#                                     min_solution$solution[3])
# 
# result_df[,ncol(result_df)+ 1] <- c(min_solution$solution[1]*costs[1],
#                                     min_solution$solution[2]*costs[2],
#                                     min_solution$solution[3]*costs[3])
# 
# result_df <- result_df %>%
#   mutate(Total_cars = result_df[,3]*result_df[,6],
#          Total_animals = result_df[,4]*result_df[,6],
#          Total_robots = result_df[,5]*result_df[,6])
# 
# 
# colnames(result_df) <- c("Factories",
#                          "Cost/day",
#                          "Cars/day",
#                          "Animals/day",
#                          "Robots/day",
#                          "Minimum days",
#                          "Minimum cost",
#                          "Cars Produced",
#                          "Animals Produced",
#                          "Robots Produced")
# 
# # Print the resulting data frame
# print(result_df)
# 

```


```{r}
# Sensitivity Analysis

min_solution$sens.coef.from
min_solution$sens.coef.to

```



## c)

```{r}
compute_weibull_stats <- function(shape, scale) {
  # Compute the mean
  mean_value <- scale * gamma(1 + (1/shape))
  
  
  # # Compute the median
  # median_value <- scale * qweibull(0.5, shape, scale)
  
  
  # Compute the median
  median_value <- scale * (log(2)^(1/shape))
  
  
  # Compute the mode
  if (shape > 1)
    mode_value <- scale * ((shape - 1) / shape)^(1/shape)
  else
    mode_value <- 0
  
  # Compute the variance
  variance_value <- scale^2 * (gamma(1 + (2/shape)) - (gamma(1 + (1/shape)))^2)
  
  # Return the computed statistics as a named list
  return(list(mean = mean_value, 
              median = median_value, 
              mode = mode_value, 
              variance = variance_value))
}

# lambda = scale
# K = shape

# Example usage
scale_param <- 6
shape_param <- 1

stats <- compute_weibull_stats(shape_param, scale_param)

# Accessing the computed statistics
mean_value <- stats$mean
median_value <- stats$median
mode_value <- stats$mode
variance_value <- stats$variance

# Printing the computed statistics
cat("Mean:", mean_value, "\n")
cat("Median:", median_value, "\n")
cat("Mode:", mode_value, "\n")
cat("Variance:", variance_value, "\n")

```

### Maximum likelihood estimator (MLE) of the parameters from the Weibull distribution



Probability Density Function $(f(x;\lambda,k))$ for the Weibull distribution:


$$
\Large f(x;\lambda,k) = 
\begin{cases}
  \Large\frac{k}{\lambda}\left(\frac{x}{\lambda}\right)^{k-1}  e^{-\left(\frac{x}{\lambda}\right)^k}  , & x \ge 0\\
  \Large0, &x < 0
\end{cases}
$$


The likelihood function $(L_{\hat{x}}(\lambda,k))$:


$$
\begin{aligned}
 \Large L_{\hat{x}}(\lambda,k) &= \Large \prod_{i=1}^{n} \frac{k}{\lambda}\left(\frac{x_i}{\lambda}\right)^{k-1}  e^{-\left(\frac{x_i}{\lambda}\right)^k} \\
 &= \Large\left(\frac{k}{\lambda}\right)^n    \left(\frac{1}{\lambda^{k-1}}\right)^n    \left(\prod_{i=1}^{n}x_i^{k-1}\right)    \left(e^{-\sum\limits_{i=1}^{n}\left(\frac{x_i}{\lambda}\right)^k}\right)  \\
 &=\Large \frac{k^n}{\lambda^{nk}}    e^{-\sum\limits_{i=1}^{n}\left(\frac{x_i}{\lambda}\right)^k}    \prod_{i=1}^{n}x_i^{k-1}    
\end{aligned}
$$


The log-likelihood function $(\ln L_{\hat{x}}(\lambda,k))$:


$$
\Large \ln L_{\hat{x}}(\lambda,k) = \Large n\ln(k) - nk\ln(\lambda) - \sum\limits_{i=1}^{n}\left(\frac{x_i}{\lambda}\right)^k + (k-1)\sum\limits_{i=1}^{n}\ln x_i
$$


Partial derivative of the log-likelihood function with respect to $\lambda$:


$$
\large \frac{\partial \ln L_{\hat{x}}(\lambda,k)}{\partial \lambda} = \Large -\frac{nk}{\lambda} + k\sum\limits_{i=1}^{n} \frac{x_i^k}{\lambda^{k+1}}
$$


Solving for the desired parameter of $\lambda$ by setting the partial derivative equal to zero:

$$
\begin{aligned}
\large \frac{\partial \ln L_{\hat{x}}(\lambda,k)}{\partial \lambda} &= \Large 0 \\
\Large -\frac{nk}{\lambda} + k\sum\limits_{i=1}^{n} \frac{x_i^k}{\lambda^{k+1}} &= \Large0 \\
\Large -\frac{nk}{\lambda} + \frac{k}{\lambda}\sum\limits_{i=1}^{n}\left(\frac{x_i}{\lambda}\right)^k & = \Large0 \\
\Large - n +\sum\limits_{i=1}^{n}\frac{x_i^k}{\lambda^k} &= \Large 0 \\
\Large \frac{1}{\lambda^k} \sum\limits_{i=1}^{n}x_i^k &= \Large n \\
\Large \frac{1}{n} \sum\limits_{i=1}^{n}x_i^k &= \Large \lambda^k
\end{aligned}
$$

Therefore the estimator $\hat{\lambda}$ is:

$$
\Large \hat{\lambda} = \Large \left( \frac{1}{n} \sum\limits_{i=1}^{n}x_i^k \right)^\frac{1}{k}
$$






Plugging in $\hat{\lambda}$ into the log-likelihood function $(\ln L_{\hat{x}}(\lambda,k))$ and then differentiating with respect to $k$ in order to find the estimator $\hat{k}$:


$$
\begin{aligned}
\large \frac{\partial \ln L_{\hat{x}}(\lambda,k)}{\partial k} &= \Large \frac{\partial}{\partial k} \left[ n\ln k - nk\ln \lambda - \sum\limits_{i=1}^{n}\left(\frac{x_i}{\lambda}\right)^k + (k-1)\sum\limits_{i=1}^{n}\ln x_i \right] \\
&= \Large \frac{\partial}{\partial k} \left[ n\ln k - nk\ln\left[ \left( \frac{1}{n} \sum\limits_{i=1}^{n}x_i^k \right)^\frac{1}{k}  \right] - \frac{\sum\limits_{i=1}^{n}x_i^k}{\left[ \left( \frac{1}{n} \sum\limits_{i=1}^{n}x_i^k \right)^\frac{1}{k} \right]^k} + (k-1)\sum\limits_{i=1}^{n}\ln x_i \right] \\
&= \Large \frac{\partial}{\partial k} \left[ n\ln k - \frac{nk}{k}\ln\left( \frac{1}{n} \sum\limits_{i=1}^{n}x_i^k \right) - \frac{\sum\limits_{i=1}^{n}x_i^k}{\left( \frac{1}{n} \sum\limits_{i=1}^{n}x_i^k \right)} + (k-1)\sum\limits_{i=1}^{n}\ln x_i \right] \\
&= \Large \frac{\partial}{\partial k} \left[ n\ln k - n\ln\left( \frac{1}{n} \sum\limits_{i=1}^{n}x_i^k \right) - n + (k-1)\sum\limits_{i=1}^{n}\ln x_i \right] \\
&= \Large \frac{n}{k} - \left( \frac{n\sum\limits_{i=1}^{n}x_i^k\ln{x_i}}{\sum\limits_{i=1}^{n}x_i^k} \right) + \sum\limits_{i=1}^{n}\ln x_i \\
&= \Large \frac{1}{k} - \left( \frac{\sum\limits_{i=1}^{n}x_i^k\ln{x_i}}{\sum\limits_{i=1}^{n}x_i^k} \right) + \frac{1}{n}\sum\limits_{i=1}^{n}\ln x_i
\end{aligned}
$$


Solving for the desired parameter of $k$ by setting the partial derivative equal to zero:

$$
\begin{aligned}
\large \frac{\partial \ln L_{\hat{x}}(\lambda,k)}{\partial k} &= \Large 0 \\
\Large \frac{1}{k} - \left( \frac{\sum\limits_{i=1}^{n}x_i^k\ln{x_i}}{\sum\limits_{i=1}^{n}x_i^k} \right) + \frac{1}{n}\sum\limits_{i=1}^{n}\ln x_i &= \Large 0 \\
\Large  \left( \frac{\sum\limits_{i=1}^{n}x_i^k\ln{x_i}}{\sum\limits_{i=1}^{n}x_i^k} \right) - \frac{1}{n}\sum\limits_{i=1}^{n}\ln x_i&= \Large \frac{1}{k}
\end{aligned}
$$



Therefore the estimator $\hat{k}$ is:

$$
\Large \hat{k} = \left[ \Large \frac{\sum\limits_{i=1}^{n}x_i^k\ln{x_i}}{\sum\limits_{i=1}^{n}x_i^k}  - \frac{1}{n}\sum\limits_{i=1}^{n}\ln x_i \right]^{-1}
$$